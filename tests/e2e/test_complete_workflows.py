"""
Comprehensive End-to-End workflow tests for TTS CLI.

This module tests complete user workflows from CLI input to audio output,
covering real-world usage scenarios and validating the entire synthesis pipeline.
"""

import pytest
import time
import tempfile
import json
import subprocess
import psutil
import os
from pathlib import Path
from unittest.mock import patch, MagicMock
from click.testing import CliRunner

from tts.cli import main as cli
from tests.utils.test_helpers import (
    validate_audio_file_comprehensive,
    estimate_audio_duration_from_text,
    create_realistic_audio_file,
    CLITestHelper,
    AudioValidationResult,
    WAVE_SUPPORT,
    SOUNDFILE_SUPPORT
)


class WorkflowTestBase:
    """Base class for workflow tests with common setup and utilities."""
    
    def setup_method(self):
        """Set up test environment for workflow tests."""
        self.runner = CliRunner()
        self.cli_helper = CLITestHelper(self.runner)
        self.start_time = time.time()
        self.workflow_metrics = {
            'synthesis_time': None,
            'validation_time': None,
            'total_time': None,
            'audio_quality': None,
            'success_rate': 0.0
        }
    
    def teardown_method(self):
        """Clean up and record workflow metrics."""
        self.workflow_metrics['total_time'] = time.time() - self.start_time
    
    def record_workflow_success(self, successful_steps: int, total_steps: int):
        """Record workflow success rate."""
        self.workflow_metrics['success_rate'] = successful_steps / total_steps if total_steps > 0 else 0.0
    
    def validate_workflow_performance(self, max_duration: float = 60.0):
        """Validate that workflow completed within performance expectations."""
        assert self.workflow_metrics['total_time'] <= max_duration, \
            f"Workflow took too long: {self.workflow_metrics['total_time']:.2f}s > {max_duration}s"
    
    def _check_network_available(self):
        """Check if network is available for real API tests."""
        try:
            import socket
            socket.create_connection(("8.8.8.8", 53), timeout=3)
            return True
        except (socket.error, OSError):
            return False


@pytest.mark.e2e
@pytest.mark.workflow
class TestBasicSynthesisWorkflows(WorkflowTestBase):
    """Test basic text-to-speech synthesis workflows."""
    
    def test_complete_text_to_audio_workflow(self, tmp_path):
        """Test complete workflow: text input → TTS synthesis → audio validation."""
        # Step 1: Prepare input text
        test_text = "This is a comprehensive end-to-end workflow test for the TTS system."
        output_file = tmp_path / "workflow_test.mp3"
        
        workflow_steps = []
        start_time = time.time()
        
        # Step 2: Synthesize audio
        synthesis_start = time.time()
        result, actual_output = self.cli_helper.invoke_save(
            test_text,
            provider="@edge",
            output_path=str(output_file),
            format="mp3"
        )
        synthesis_time = time.time() - synthesis_start
        self.workflow_metrics['synthesis_time'] = synthesis_time
        workflow_steps.append(("synthesis", result.exit_code == 0))
        
        # Step 3: Validate file creation
        file_exists = actual_output.exists() if actual_output else False
        workflow_steps.append(("file_creation", file_exists))
        
        if not file_exists:
            pytest.skip("Synthesis failed or provider not available")
        
        # Step 4: Validate audio properties
        validation_start = time.time()
        validation_result = validate_audio_file_comprehensive(
            actual_output,
            expected_format="mp3",
            min_duration=2.0,
            max_duration=20.0,
            min_file_size=1000,
            check_silence=True
        )
        validation_time = time.time() - validation_start
        self.workflow_metrics['validation_time'] = validation_time
        workflow_steps.append(("audio_validation", validation_result.valid))
        
        # Step 5: Verify audio content quality
        content_quality_checks = []
        if validation_result.valid:
            # Duration should match text length expectations
            estimated_duration = estimate_audio_duration_from_text(test_text, wpm=150)
            duration_ratio = validation_result.duration / estimated_duration if estimated_duration > 0 else 0
            content_quality_checks.append(("duration_accuracy", 0.5 <= duration_ratio <= 2.5))
            
            # File should not be silent
            if validation_result.has_silence is not None:
                content_quality_checks.append(("not_silent", not validation_result.has_silence))
            
            # File size should be reasonable
            content_quality_checks.append(("reasonable_size", validation_result.file_size > 1000))
        
        workflow_steps.extend(content_quality_checks)
        
        # Calculate success metrics
        successful_steps = sum(1 for _, success in workflow_steps if success)
        self.record_workflow_success(successful_steps, len(workflow_steps))
        
        # Assert workflow success
        assert result.exit_code == 0, f"CLI command failed: {result.output}"
        assert file_exists, "Audio file was not created"
        assert validation_result.valid, f"Audio validation failed: {validation_result.error}"
        assert self.workflow_metrics['success_rate'] >= 0.8, \
            f"Workflow success rate too low: {self.workflow_metrics['success_rate']:.2f}"
        
        # Performance assertions
        assert synthesis_time < 30.0, f"Synthesis took too long: {synthesis_time:.2f}s"
        assert validation_time < 5.0, f"Validation took too long: {validation_time:.2f}s"
        self.validate_workflow_performance(45.0)
    
    def test_multiple_format_synthesis_workflow(self, tmp_path):
        """Test workflow with multiple output formats."""
        # Skip real API tests in CI environment
        if os.getenv("CI") or os.getenv("GITHUB_ACTIONS") or not self._check_network_available():
            pytest.skip("Skipping real API test in CI environment")
            
        test_text = "Multi-format workflow test"
        formats = ["mp3", "wav", "ogg"]
        
        workflow_results = {}
        successful_formats = 0
        
        for format_name in formats:
            output_file = tmp_path / f"multiformat_test.{format_name}"
            
            # Synthesize
            result, actual_output = self.cli_helper.invoke_save(
                test_text,
                provider="@edge",
                output_path=str(output_file),
                format=format_name
            )
            
            if result.exit_code == 0 and actual_output and actual_output.exists():
                # Validate
                validation_result = validate_audio_file_comprehensive(
                    actual_output,
                    expected_format=format_name,
                    min_duration=1.0,
                    max_duration=10.0,
                    min_file_size=500
                )
                
                workflow_results[format_name] = {
                    'synthesis_success': True,
                    'validation_result': validation_result,
                    'file_size': actual_output.stat().st_size if actual_output.exists() else 0
                }
                
                if validation_result.valid:
                    successful_formats += 1
            else:
                workflow_results[format_name] = {
                    'synthesis_success': False,
                    'validation_result': None,
                    'file_size': 0
                }
        
        # At least one format should work
        assert successful_formats > 0, "No formats produced valid audio"
        
        # Compare format characteristics
        if successful_formats > 1:
            valid_formats = [
                (fmt, data) for fmt, data in workflow_results.items() 
                if data['synthesis_success'] and data['validation_result'] and data['validation_result'].valid
            ]
            
            if len(valid_formats) > 1:
                durations = [
                    data['validation_result'].duration 
                    for _, data in valid_formats 
                    if data['validation_result'].duration
                ]
                
                # Durations should be similar across formats
                if len(durations) > 1:
                    avg_duration = sum(durations) / len(durations)
                    for duration in durations:
                        variance = abs(duration - avg_duration) / avg_duration
                        assert variance < 0.2, f"Duration variance {variance} too high across formats"
        
        self.record_workflow_success(successful_formats, len(formats))
        assert self.workflow_metrics['success_rate'] >= 0.33, "Too few formats succeeded"
    
    def test_streaming_vs_save_workflow_comparison(self, tmp_path):
        """Test and compare streaming vs save workflows."""
        test_text = "Workflow comparison test between streaming and save operations"
        
        # Test save workflow
        output_file = tmp_path / "save_workflow_test.mp3"
        save_start_time = time.time()
        
        save_result, actual_output = self.cli_helper.invoke_save(
            test_text,
            provider="@edge",
            output_path=str(output_file),
            voice="en-US-AvaNeural"
        )
        save_duration = time.time() - save_start_time
        
        # Test streaming workflow (simulate by capturing output)
        stream_start_time = time.time()
        stream_result = self.runner.invoke(cli, ["speak", "@edge", test_text, "--voice", "en-US-AvaNeural"])
        stream_duration = time.time() - stream_start_time
        
        # Validate save workflow results
        save_success = False
        if save_result.exit_code == 0 and actual_output and actual_output.exists():
            validation_result = validate_audio_file_comprehensive(
                actual_output,
                expected_format="mp3",
                min_duration=1.0,
                max_duration=15.0,
                min_file_size=800
            )
            save_success = validation_result.valid
        
        # Validate streaming workflow (basic success check)
        stream_success = stream_result.exit_code == 0
        
        # Performance comparison
        workflow_metrics = {
            'save_duration': save_duration,
            'stream_duration': stream_duration,
            'save_success': save_success,
            'stream_success': stream_success
        }
        
        # At least one workflow should succeed
        assert save_success or stream_success, "Both workflows failed"
        
        # If both succeeded, compare performance
        if save_success and stream_success:
            # Streaming should generally be faster (no file I/O overhead)
            assert stream_duration <= save_duration * 1.5, \
                f"Streaming workflow unexpectedly slower: {stream_duration:.2f}s vs {save_duration:.2f}s"
        
        # Both workflows should complete in reasonable time
        assert save_duration < 45.0, f"Save workflow too slow: {save_duration:.2f}s"
        assert stream_duration < 30.0, f"Stream workflow too slow: {stream_duration:.2f}s"
        
        self.record_workflow_success(
            int(save_success) + int(stream_success), 
            2
        )


@pytest.mark.e2e
@pytest.mark.workflow
class TestDocumentProcessingWorkflows(WorkflowTestBase):
    """Test document-to-speech processing workflows."""
    
    def test_html_to_speech_complete_workflow(self, tmp_path):
        """Test complete HTML document processing workflow."""
        # Create test HTML document
        html_content = """
        <html>
        <head><title>Test Document</title></head>
        <body>
            <h1>Welcome to TTS Testing</h1>
            <p>This is a <strong>comprehensive test</strong> of HTML to speech conversion.</p>
            <p>The system should handle various HTML elements correctly.</p>
            <ul>
                <li>Process text content</li>
                <li>Handle formatting</li>
                <li>Maintain document structure</li>
            </ul>
        </body>
        </html>
        """
        
        html_file = tmp_path / "test_document.html"
        html_file.write_text(html_content)
        output_file = tmp_path / "html_workflow_test.wav"
        
        workflow_steps = []
        
        # Step 1: Process document
        result = self.runner.invoke(cli, [
            "document", str(html_file),
            "--doc-format", "html",
            "--emotion-profile", "technical",
            "--save",
            "--output", str(output_file),
            "--format", "wav"
        ])
        
        workflow_steps.append(("document_processing", result.exit_code == 0))
        
        # Step 2: Validate file creation
        file_exists = output_file.exists()
        workflow_steps.append(("file_creation", file_exists))
        
        if not file_exists:
            # Try to find actual output file (CLI might use different naming)
            potential_files = list(tmp_path.glob("*.wav"))
            if potential_files:
                output_file = potential_files[0]
                file_exists = True
                workflow_steps[-1] = ("file_creation", True)
        
        if not file_exists:
            pytest.skip("Document processing failed or provider not available")
        
        # Step 3: Validate audio properties
        validation_result = validate_audio_file_comprehensive(
            output_file,
            expected_format="wav",
            min_duration=5.0,  # HTML should produce substantial audio
            max_duration=60.0,
            min_file_size=20000,
            check_silence=True
        )
        workflow_steps.append(("audio_validation", validation_result.valid))
        
        # Step 4: Validate content processing
        if validation_result.valid:
            # Duration should be appropriate for content length
            content_text = "Welcome to TTS Testing This is a comprehensive test of HTML to speech conversion. The system should handle various HTML elements correctly. Process text content Handle formatting Maintain document structure"
            estimated_duration = estimate_audio_duration_from_text(content_text, wpm=140)  # Slightly slower for structured content
            
            if validation_result.duration and estimated_duration > 0:
                duration_ratio = validation_result.duration / estimated_duration
                workflow_steps.append(("duration_accuracy", 0.4 <= duration_ratio <= 3.0))
            
            # Should not be silent
            if validation_result.has_silence is not None:
                workflow_steps.append(("not_silent", not validation_result.has_silence))
        
        successful_steps = sum(1 for _, success in workflow_steps if success)
        self.record_workflow_success(successful_steps, len(workflow_steps))
        
        # Workflow assertions
        assert result.exit_code == 0, f"Document processing failed: {result.output}"
        assert file_exists, "Audio output file was not created"
        assert validation_result.valid, f"Audio validation failed: {validation_result.error}"
        assert self.workflow_metrics['success_rate'] >= 0.75, \
            f"HTML workflow success rate too low: {self.workflow_metrics['success_rate']:.2f}"
    
    def test_markdown_to_speech_workflow_with_ssml(self, tmp_path):
        """Test Markdown document processing with SSML generation."""
        # Create test Markdown document
        markdown_content = """
        # TTS Markdown Test
        
        This is a **comprehensive test** of Markdown to speech conversion.
        
        ## Features Being Tested
        
        1. Header processing
        2. **Bold text** emphasis
        3. *Italic text* styling
        4. Code blocks and `inline code`
        
        ### Code Example
        
        ```python
        def hello_world():
            print("Hello, TTS world!")
        ```
        
        ## Conclusion
        
        The system should handle all these elements effectively.
        """
        
        md_file = tmp_path / "test_document.md"
        md_file.write_text(markdown_content)
        output_file = tmp_path / "markdown_workflow_test.mp3"
        
        # Process with SSML generation
        result = self.runner.invoke(cli, [
            "document", str(md_file),
            "--doc-format", "markdown",
            "--ssml-platform", "azure",
            "--emotion-profile", "tutorial",
            "--save",
            "--output", str(output_file)
        ])
        
        workflow_success = []
        
        # Validate processing
        workflow_success.append(result.exit_code == 0)
        
        # Check file creation
        file_exists = output_file.exists()
        if not file_exists:
            # Check for alternative output files
            potential_files = list(tmp_path.glob("*.mp3"))
            if potential_files:
                output_file = potential_files[0]
                file_exists = True
        
        workflow_success.append(file_exists)
        
        if file_exists:
            # Validate audio
            validation_result = validate_audio_file_comprehensive(
                output_file,
                expected_format="mp3",
                min_duration=8.0,  # Markdown content should be substantial
                max_duration=90.0,
                min_file_size=5000
            )
            workflow_success.append(validation_result.valid)
            
            # Check for reasonable duration
            if validation_result.duration:
                workflow_success.append(validation_result.duration >= 8.0)
        
        success_rate = sum(workflow_success) / len(workflow_success)
        self.record_workflow_success(sum(workflow_success), len(workflow_success))
        
        if not file_exists:
            pytest.skip("Markdown processing failed or provider not available")
        
        assert result.exit_code == 0, f"Markdown processing failed: {result.output}"
        assert success_rate >= 0.75, f"Markdown workflow success rate too low: {success_rate:.2f}"
    
    def test_json_to_speech_structured_workflow(self, tmp_path):
        """Test JSON document processing workflow."""
        # Create test JSON document
        json_content = {
            "report": {
                "title": "TTS System Performance Report",
                "date": "2024-03-15",
                "summary": "Overall system performance is excellent",
                "metrics": {
                    "synthesis_speed": "2.5x real-time",
                    "audio_quality": "High fidelity",
                    "success_rate": "98.5%"
                },
                "recommendations": [
                    "Continue monitoring performance",
                    "Optimize for edge cases",
                    "Expand format support"
                ]
            }
        }
        
        json_file = tmp_path / "test_report.json"
        json_file.write_text(json.dumps(json_content, indent=2))
        output_file = tmp_path / "json_workflow_test.wav"
        
        # Process JSON document
        result = self.runner.invoke(cli, [
            "document", str(json_file),
            "--doc-format", "json",
            "--emotion-profile", "technical",
            "--save",
            "--output", str(output_file),
            "--format", "wav"
        ])
        
        workflow_metrics = {
            'processing_success': result.exit_code == 0,
            'file_creation': False,
            'audio_validation': False,
            'duration_appropriate': False
        }
        
        # Check file creation
        if output_file.exists():
            workflow_metrics['file_creation'] = True
        else:
            # Check for alternative output files
            potential_files = list(tmp_path.glob("*.wav"))
            if potential_files:
                output_file = potential_files[0]
                workflow_metrics['file_creation'] = True
        
        if workflow_metrics['file_creation']:
            # Validate audio output
            validation_result = validate_audio_file_comprehensive(
                output_file,
                expected_format="wav",
                min_duration=10.0,  # JSON content should produce substantial audio
                max_duration=120.0,
                min_file_size=40000  # WAV files should be larger
            )
            
            workflow_metrics['audio_validation'] = validation_result.valid
            
            # Check duration appropriateness for structured data
            if validation_result.duration:
                workflow_metrics['duration_appropriate'] = validation_result.duration >= 10.0
        
        successful_steps = sum(1 for success in workflow_metrics.values() if success)
        self.record_workflow_success(successful_steps, len(workflow_metrics))
        
        if not workflow_metrics['file_creation']:
            pytest.skip("JSON processing failed or provider not available")
        
        assert workflow_metrics['processing_success'], f"JSON processing failed: {result.output}"
        assert workflow_metrics['file_creation'], "JSON processing did not create audio file"
        assert self.workflow_metrics['success_rate'] >= 0.5, \
            f"JSON workflow success rate too low: {self.workflow_metrics['success_rate']:.2f}"


@pytest.mark.e2e
@pytest.mark.workflow
@pytest.mark.slow
class TestComplexWorkflowScenarios(WorkflowTestBase):
    """Test complex, realistic workflow scenarios."""
    
    def test_multi_step_content_processing_workflow(self, tmp_path):
        """Test workflow with multiple content processing steps."""
        # Skip real API tests in CI environment
        if os.getenv("CI") or os.getenv("GITHUB_ACTIONS") or not self._check_network_available():
            pytest.skip("Skipping real API test in CI environment")
        # Create complex document with multiple sections
        complex_content = """
        # TTS Integration Guide
        
        ## Overview
        This guide covers comprehensive TTS integration patterns.
        
        ## Quick Start
        1. Install the TTS CLI: `pip install goobits-tts`
        2. Configure your preferred provider
        3. Test basic synthesis: `tts "Hello, world!"`
        
        ## Advanced Usage
        
        ### Provider Configuration
        ```bash
        tts config set openai_api_key YOUR_KEY
        tts config set voice openai_tts:alloy
        ```
        
        ### Document Processing
        Process various formats:
        - HTML documents
        - Markdown files
        - JSON reports
        
        ## Performance Considerations
        
        The system provides excellent performance with:
        - Real-time synthesis for short texts
        - Batch processing for large documents
        - Concurrent request handling
        
        ## Troubleshooting
        
        Common issues and solutions:
        1. **Audio quality**: Check sample rate settings
        2. **Speed**: Use appropriate provider for use case
        3. **Compatibility**: Verify format support
        """
        
        content_file = tmp_path / "complex_guide.md"
        content_file.write_text(complex_content)
        
        # Multi-step workflow
        workflow_results = {}
        
        # Step 1: Process with technical emotion profile
        technical_output = tmp_path / "technical_guide.mp3"
        technical_result = self.runner.invoke(cli, [
            "document", str(content_file),
            "--emotion-profile", "technical",
            "--save",
            "--output", str(technical_output)
        ])
        workflow_results['technical_processing'] = technical_result.exit_code == 0
        
        # Step 2: Process with tutorial emotion profile
        tutorial_output = tmp_path / "tutorial_guide.wav"
        tutorial_result = self.runner.invoke(cli, [
            "document", str(content_file),
            "--emotion-profile", "tutorial",
            "--save",
            "--output", str(tutorial_output),
            "--format", "wav"
        ])
        workflow_results['tutorial_processing'] = tutorial_result.exit_code == 0
        
        # Step 3: Generate SSML version
        ssml_output = tmp_path / "ssml_guide.mp3"
        ssml_result = self.runner.invoke(cli, [
            "document", str(content_file),
            "--ssml-platform", "azure",
            "--save",
            "--output", str(ssml_output)
        ])
        workflow_results['ssml_processing'] = ssml_result.exit_code == 0
        
        # Validate outputs
        output_validations = {}
        
        for name, output_file in [
            ('technical', technical_output),
            ('tutorial', tutorial_output),
            ('ssml', ssml_output)
        ]:
            if output_file.exists():
                validation_result = validate_audio_file_comprehensive(
                    output_file,
                    expected_format=output_file.suffix[1:],  # Remove dot from extension
                    min_duration=15.0,  # Complex content should be substantial
                    max_duration=180.0,
                    min_file_size=10000
                )
                output_validations[name] = validation_result.valid
            else:
                # Check for alternative files
                alt_files = list(tmp_path.glob(f"*{name}*"))
                if alt_files:
                    validation_result = validate_audio_file_comprehensive(
                        alt_files[0],
                        min_duration=15.0,
                        max_duration=180.0,
                        min_file_size=10000
                    )
                    output_validations[name] = validation_result.valid
                else:
                    output_validations[name] = False
        
        # Calculate overall success
        all_results = {**workflow_results, **output_validations}
        successful_steps = sum(1 for success in all_results.values() if success)
        self.record_workflow_success(successful_steps, len(all_results))
        
        # Assertions
        assert successful_steps >= len(all_results) * 0.5, \
            f"Multi-step workflow success rate too low: {self.workflow_metrics['success_rate']:.2f}"
        
        # At least one processing step should succeed
        processing_success = any([
            workflow_results['technical_processing'],
            workflow_results['tutorial_processing'],
            workflow_results['ssml_processing']
        ])
        assert processing_success, "All document processing steps failed"
        
        # At least one validation should succeed
        validation_success = any(output_validations.values())
        assert validation_success, "All audio validations failed"
    
    def test_error_recovery_workflow(self, tmp_path):
        """Test workflow error recovery and fallback mechanisms."""
        # Test various error scenarios and recovery
        error_scenarios = [
            {
                'name': 'invalid_provider',
                'command': ["speak", "@invalid_provider", "test"],
                'expected_behavior': 'graceful_error'
            },
            {
                'name': 'missing_output_dir',
                'command': ["save", "test", "-o", str(tmp_path / "nonexistent" / "output.mp3")],
                'expected_behavior': 'directory_creation_or_error'
            },
            {
                'name': 'invalid_format',
                'command': ["save", "test", "-f", "invalid_format", "-o", str(tmp_path / "test.mp3")],
                'expected_behavior': 'format_error'
            },
            {
                'name': 'empty_document',
                'setup': lambda: (tmp_path / "empty.md").write_text(""),
                'command': ["document", str(tmp_path / "empty.md")],
                'expected_behavior': 'empty_content_handling'
            }
        ]
        
        recovery_results = {}
        
        for scenario in error_scenarios:
            # Setup if needed
            if 'setup' in scenario:
                scenario['setup']()
            
            # Execute command
            result = self.runner.invoke(cli, scenario['command'])
            
            # Analyze recovery behavior
            recovery_analysis = {
                'exit_code': result.exit_code,
                'has_error_message': 'error' in result.output.lower() or 'Error' in result.output,
                'graceful_handling': result.exit_code in [0, 1, 2],  # Expected error codes
                'provides_guidance': any(keyword in result.output.lower() for keyword in [
                    'usage', 'help', 'available', 'try', 'check'
                ])
            }
            
            recovery_results[scenario['name']] = recovery_analysis
        
        # Evaluate recovery quality
        recovery_scores = []
        for scenario_name, analysis in recovery_results.items():
            score = 0
            if analysis['graceful_handling']:
                score += 0.4
            if analysis['has_error_message']:
                score += 0.3
            if analysis['provides_guidance']:
                score += 0.3
            recovery_scores.append(score)
        
        avg_recovery_score = sum(recovery_scores) / len(recovery_scores) if recovery_scores else 0
        self.record_workflow_success(int(avg_recovery_score * len(error_scenarios)), len(error_scenarios))
        
        # Assert recovery quality
        assert avg_recovery_score >= 0.6, f"Error recovery quality too low: {avg_recovery_score:.2f}"
        
        # All scenarios should handle errors gracefully (no crashes)
        for scenario_name, analysis in recovery_results.items():
            assert analysis['graceful_handling'], \
                f"Scenario '{scenario_name}' did not handle error gracefully"
    
    @pytest.mark.skipif(not os.getenv("TEST_PERFORMANCE_WORKFLOWS"), 
                       reason="Performance workflow testing disabled")
    def test_performance_stress_workflow(self, tmp_path):
        """Test workflow performance under stress conditions."""
        import threading
        import queue
        
        # Create multiple processing tasks
        tasks = [
            {
                'text': f"Performance stress test number {i}. This is a medium-length text to test concurrent processing capabilities.",
                'output': tmp_path / f"stress_test_{i}.mp3",
                'provider': "@edge"
            }
            for i in range(5)
        ]
        
        # Process tasks concurrently
        results_queue = queue.Queue()
        start_time = time.time()
        
        def process_task(task):
            task_start = time.time()
            result, actual_output = self.cli_helper.invoke_save(
                task['text'],
                provider=task['provider'],
                output_path=str(task['output'])
            )
            task_duration = time.time() - task_start
            
            validation_result = None
            if result.exit_code == 0 and actual_output and actual_output.exists():
                validation_result = validate_audio_file_comprehensive(
                    actual_output,
                    expected_format="mp3",
                    min_duration=2.0,
                    max_duration=20.0,
                    min_file_size=1000
                )
            
            results_queue.put({
                'task': task,
                'result': result,
                'duration': task_duration,
                'validation': validation_result,
                'success': result.exit_code == 0 and validation_result and validation_result.valid
            })
        
        # Start concurrent processing
        threads = []
        for task in tasks:
            thread = threading.Thread(target=process_task, args=(task,))
            thread.start()
            threads.append(thread)
        
        # Wait for completion
        for thread in threads:
            thread.join(timeout=60)  # 1 minute timeout per task
        
        total_time = time.time() - start_time
        
        # Collect results
        task_results = []
        while not results_queue.empty():
            task_results.append(results_queue.get())
        
        # Analyze performance
        successful_tasks = [r for r in task_results if r['success']]
        task_durations = [r['duration'] for r in task_results]
        
        performance_metrics = {
            'total_time': total_time,
            'avg_task_duration': sum(task_durations) / len(task_durations) if task_durations else 0,
            'success_rate': len(successful_tasks) / len(tasks),
            'concurrent_efficiency': len(tasks) / total_time if total_time > 0 else 0
        }
        
        self.record_workflow_success(len(successful_tasks), len(tasks))
        
        # Performance assertions
        assert total_time < 120.0, f"Concurrent processing took too long: {total_time:.2f}s"
        assert performance_metrics['success_rate'] >= 0.6, \
            f"Concurrent workflow success rate too low: {performance_metrics['success_rate']:.2f}"
        assert performance_metrics['avg_task_duration'] < 30.0, \
            f"Average task duration too high: {performance_metrics['avg_task_duration']:.2f}s"
        
        # Should process at least 0.05 tasks per second (reasonable concurrency)
        assert performance_metrics['concurrent_efficiency'] >= 0.05, \
            f"Concurrent efficiency too low: {performance_metrics['concurrent_efficiency']:.3f} tasks/s"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])